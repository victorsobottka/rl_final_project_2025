{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9ff25d",
   "metadata": {},
   "source": [
    "## Team Schedule Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d20e6",
   "metadata": {},
   "source": [
    "| Day       | Task                                                                    |\n",
    "| --------- | ----------------------------------------------------------------------- |\n",
    "| **Day 1** | Set up environment skeleton (Gym-compatible)                            |\n",
    "| **Day 2** | Implement simulator: synthetic or LOB-based market                      |\n",
    "| **Day 3** | Add SAC with stable-baselines3 + training loop                          |\n",
    "| **Day 4** | Tune rewards (e.g., inventory risk penalty, cost)                       |\n",
    "| **Day 5** | Run training, collect plots, evaluate vs VWAP/TWAP                      |\n",
    "| **Day 6** | Polish results: plots, metrics, ablation (e.g., high vs low volatility) |\n",
    "| **Day 7** | Finalize slides + notebook/code repo for presentation                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff44b3",
   "metadata": {},
   "source": [
    "## **Optimal Trade Execution using Soft Actor-Critic (SAC) in a Realistic Limit Order Book Simulator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77bbd00",
   "metadata": {},
   "source": [
    "#### **Project Goal**\n",
    "\n",
    "Simulate the behavior of a trader executing a large order (buy or sell) over a short time horizon (like 60 minutes), and use Soft Actor-Critic (SAC) to minimize execution cost while respecting market impact and inventory risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10721c",
   "metadata": {},
   "source": [
    "#### **SAC Review**\n",
    "\n",
    "Literature review about SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da870ac3",
   "metadata": {},
   "source": [
    "#### **Environment**\n",
    "\n",
    "Build or use a Gym-style custom environment with:\n",
    "\n",
    "- state: window of historical mid-prices, spread, inventory, time left\n",
    "\n",
    "- action: % of remaining order to execute now (continuous: 0–1)\n",
    "\n",
    "- reward: negative cost (slippage + penalty for unfilled inventory)\n",
    "\n",
    "Bonus realism:\n",
    "\n",
    "- Add adverse selection penalty (price moves against you if you trade aggressively)\n",
    "\n",
    "- Use synthetic LOB data (mid-price + random walk) or Lobster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f95f90",
   "metadata": {},
   "source": [
    "#### **RL Agent**\n",
    "\n",
    "Train a Soft Actor-Critic (SAC) agent to:\n",
    "\n",
    "- Learn a trade-off between trading speed and cost\n",
    "\n",
    "- Adapt to different market volatility levels\n",
    "\n",
    "- Beat a baseline (e.g., VWAP or TWAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba4f9f",
   "metadata": {},
   "source": [
    "#### **Evaluation**\n",
    "\n",
    "Compare SAC vs baselines on:\n",
    "\n",
    "- Execution cost\n",
    "\n",
    "- Remaining inventory\n",
    "\n",
    "- Price slippage\n",
    "\n",
    "- Reward curve over episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99e599",
   "metadata": {},
   "source": [
    "#### **Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d733fa0",
   "metadata": {},
   "source": [
    "**Install Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stable-baselines3[extra] gym numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b932f",
   "metadata": {},
   "source": [
    "**Custom Trading Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class ExecutionEnv(gym.Env):\n",
    "    def __init__(self, total_steps=60, total_inventory=1000, spread=0.02):\n",
    "        super(ExecutionEnv, self).__init__()\n",
    "\n",
    "        self.total_steps = total_steps\n",
    "        self.total_inventory = total_inventory\n",
    "        self.remaining_inventory = total_inventory\n",
    "        self.spread = spread  # 2 cent spread for a $100 stock\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.price_series = self._generate_price_series()\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32  # price, inventory, time, volatility\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def _generate_price_series(self):\n",
    "        # Simulate a random walk price series starting at 100\n",
    "        prices = [100]\n",
    "        for _ in range(self.total_steps):\n",
    "            prices.append(prices[-1] + np.random.normal(0, 0.2))  # 20c volatility\n",
    "        return np.array(prices)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        price = self.price_series[self.current_step]\n",
    "        time_fraction = 1 - self.current_step / self.total_steps\n",
    "        inventory_fraction = self.remaining_inventory / self.total_inventory\n",
    "        volatility = np.std(self.price_series[max(0, self.current_step - 5):self.current_step + 1])\n",
    "        return np.array([price, inventory_fraction, time_fraction, volatility], dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.price_series = self._generate_price_series()\n",
    "        self.remaining_inventory = self.total_inventory\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        # action ∈ [0, 1] representing % of remaining inventory to execute this step\n",
    "        action = np.clip(action, 0, 1)[0]\n",
    "        executed_volume = action * self.remaining_inventory\n",
    "        self.remaining_inventory -= executed_volume\n",
    "\n",
    "        # Execute at the ask (we are buying)\n",
    "        mid_price = self.price_series[self.current_step]\n",
    "        ask_price = mid_price + self.spread / 2\n",
    "\n",
    "        # Cost is price * volume\n",
    "        cost = executed_volume * ask_price\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.total_steps or self.remaining_inventory <= 0\n",
    "\n",
    "        # Reward is negative cost; add penalty for unexecuted inventory\n",
    "        reward = -cost\n",
    "        if done and self.remaining_inventory > 0:\n",
    "            penalty = self.remaining_inventory * (mid_price + self.spread) * 1.5  # force-fulfill penalty\n",
    "            reward -= penalty\n",
    "\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step {self.current_step} - Remaining Inventory: {self.remaining_inventory}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913acd4",
   "metadata": {},
   "source": [
    "**Train SAC Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from execution_env import ExecutionEnv\n",
    "\n",
    "# Wrap the environment\n",
    "env = make_vec_env(lambda: ExecutionEnv(), n_envs=1)\n",
    "\n",
    "# Create SAC model\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=20000)\n",
    "\n",
    "# Save\n",
    "model.save(\"sac_execution_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56e960",
   "metadata": {},
   "source": [
    "**Evaluation + VWAP Benchmark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from execution_env import ExecutionEnv\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "env = ExecutionEnv()\n",
    "model = SAC.load(\"sac_execution_agent\")\n",
    "\n",
    "obs = env.reset()\n",
    "total_cost = 0\n",
    "executed_volume = 0\n",
    "costs = []\n",
    "prices = []\n",
    "\n",
    "while True:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    mid = env.price_series[env.current_step]\n",
    "    ask = mid + env.spread / 2\n",
    "    costs.append(-reward)\n",
    "    prices.append(mid)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# VWAP Benchmark\n",
    "vwap_cost = sum(env.price_series[i] + env.spread / 2 for i in range(env.total_steps)) * env.total_inventory / env.total_steps\n",
    "\n",
    "print(f\"SAC Execution Total Cost: ${sum(costs):.2f}\")\n",
    "print(f\"VWAP Cost (baseline):     ${vwap_cost:.2f}\")\n",
    "\n",
    "plt.plot(prices, label=\"Mid Price\")\n",
    "plt.title(\"Price Series During Execution\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa56fe0",
   "metadata": {},
   "source": [
    "#### **Output (Report)**\n",
    "\n",
    "- Motivation: Execution is the backbone of quant trading\n",
    "\n",
    "- Problem: Minimize cost for large orders\n",
    "\n",
    "- RL Model: SAC on market simulator\n",
    "\n",
    "- Results: Cost reduction vs baselines\n",
    "\n",
    "- Insight: RL learns smart pacing; adapts to market regime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a811a",
   "metadata": {},
   "source": [
    "In this project, I develop a Soft Actor-Critic (SAC) agent to learn optimal trade execution strategies in a simulated limit order book environment. The agent minimizes execution costs and risk while trading over short horizons. Compared to VWAP and TWAP benchmarks, the SAC agent shows superior adaptability under market volatility and slippage constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67494d60",
   "metadata": {},
   "source": [
    "#### **Next Steps & Add-ons**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417bca2",
   "metadata": {},
   "source": [
    "| Feature                          | Code Needed? | Value                    |\n",
    "| -------------------------------- | ------------ | ------------------------ |\n",
    "| Inventory penalty tune           | Yes          | More risk-aware behavior |\n",
    "| Market regime switching          | Small tweak  | Show SAC adaptability    |\n",
    "| Add short-selling (sell orders)  | Moderate     | Add realism              |\n",
    "| Real price data (e.g., BTC, ETH) | Minor change | Realistic simulation     |\n",
    "| Multi-agent environment          | Advanced     | Simulate market response |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
